{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code complexity analysis\n",
    "We target programming languages found in the definition file provided by GitHub: https://github.com/github-linguist/linguist/blob/main/lib/linguist/languages.yml.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading input CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input CSV\n",
    "input_file = '../csvs/cloc_summary.csv'\n",
    "raw_df = pd.read_csv(input_file)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy this DataFrame to convert the data type of the columns to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.copy()\n",
    "df['files'] = pd.to_numeric(df['files'], errors='coerce')\n",
    "df['blank'] = pd.to_numeric(df['blank'], errors='coerce')\n",
    "df['comment'] = pd.to_numeric(df['comment'], errors='coerce')\n",
    "df['code'] = pd.to_numeric(df['code'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_languages = set()\n",
    "filepath = '../languages.yml'\n",
    "if not os.path.isfile(filepath):\n",
    "    print(f\"Error: Languages YAML file not found at: {filepath}\")\n",
    "try:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        programming_languages = {lang for lang, details in data.items() \n",
    "                        if isinstance(details, dict) and details.get('type') == 'programming'}\n",
    "        print(f\"âœ… Loaded {len(programming_languages)} programming languages from {filepath}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading or parsing YAML file: {e}\")\n",
    "programming_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['files', 'blank', 'comment', 'code']\n",
    "\n",
    "# Process Each Project Individually\n",
    "processed_projects = []\n",
    "for project_id in df['project_id'].unique():\n",
    "    project_df = df[df['project_id'] == project_id]\n",
    "    project_prog_df = project_df[project_df['language'].isin(programming_languages)].copy()\n",
    "    \n",
    "    if not project_prog_df.empty:\n",
    "        new_sum = project_prog_df[numeric_cols].sum().to_dict()\n",
    "        new_sum['project_id'] = project_id\n",
    "        new_sum['language'] = 'SUM'\n",
    "        sum_row = pd.DataFrame([new_sum])\n",
    "        processed_projects.append(pd.concat([project_prog_df, sum_row]))\n",
    "    else:\n",
    "        zero_row_data = {\n",
    "            'project_id': [project_id, project_id],\n",
    "            'language': [pd.NA, 'SUM'],\n",
    "            'files': [0, 0], 'blank': [0, 0], 'comment': [0, 0], 'code': [0, 0]\n",
    "        }\n",
    "        processed_projects.append(pd.DataFrame(zero_row_data))\n",
    "\n",
    "# Assemble and Sort the Final DataFrame ---\n",
    "df_final = pd.concat(processed_projects, ignore_index=True)\n",
    "\n",
    "# Replace key function with a more robust sorting method\n",
    "# Create a temporary column to define a clear sort priority for row types.\n",
    "# Priority: 0 for NaN placeholder, 1 for actual languages, 2 for SUM rows.\n",
    "conditions = [\n",
    "    df_final['language'].isna(),\n",
    "    df_final['language'] == 'SUM'\n",
    "]\n",
    "choices = [0, 2] # 0 for NaN, 2 for SUM\n",
    "df_final['lang_order'] = np.select(conditions, choices, default=1) # 1 for everything else\n",
    "\n",
    "# Perform a stable, multi-level sort and then remove the helper column\n",
    "df_final = df_final.sort_values(\n",
    "    by=['project_id', 'lang_order', 'language']\n",
    ").drop(columns='lang_order').reset_index(drop=True)\n",
    "\n",
    "# Display Result\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_programming = df['language'].isin(programming_languages)\n",
    "# if there is a project_id that has no programming languages, keep the project_id but with language counts as na and all counts as 0\n",
    "df_filtered = df[is_programming].copy()\n",
    "\n",
    "# Recalculate SUMs\n",
    "# Group by project and sum the numeric columns to create the new SUM rows\n",
    "numeric_cols = ['files', 'blank', 'comment', 'code']\n",
    "df_sums = df_filtered.groupby('project_id')[numeric_cols].sum().reset_index()\n",
    "\n",
    "# Add the 'language' column to mark these as SUM rows\n",
    "df_sums['language'] = 'SUM'\n",
    "\n",
    "# Combine and Format the Final DataFrame\n",
    "\n",
    "# Append the new SUM rows to the filtered programming language rows\n",
    "df_final = pd.concat([df_filtered, df_sums], ignore_index=True)\n",
    "\n",
    "# Sort the results to keep projects grouped together, with SUM at the end.\n",
    "df_final = df_final.sort_values(\n",
    "    by=['project_id', 'language'],\n",
    "    key=lambda col: col.apply(lambda x: 'ZZZ' if x == 'SUM' else x) # Trick to sort SUM last\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Reorder columns to match the original format\n",
    "df_final = df_final[['project_id', 'language', 'files', 'blank', 'comment', 'code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new DataFrame to a new CSV\n",
    "output_file = '../temp_data/revised_loc_analysis.csv'\n",
    "df_final.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_final[df_final['language'] != 'SUM']\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by 'language' and aggregate the sum of columns\n",
    "sums_per_language = filtered_df.groupby('language').agg({\n",
    "    'files': 'sum',\n",
    "    'code': 'sum'\n",
    "}).reset_index()\n",
    "# calculate the percentage of each language based on the 'code' column\n",
    "sums_per_language['percentage'] = (sums_per_language['code'] / sums_per_language['code'].sum()) * 100\n",
    "sums_per_language\n",
    "sums_per_language.sort_values(by='code', ascending=False, inplace=True)\n",
    "sums_per_language.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_languages = [\"JavaScript\", \"TypeScript\", \"C#\", \"PowerShell\", \"Ruby\", \"Java\", \"Python\", \"Go\", \"PHP\", \"CSS\", \"Rust\", \"Cython\", \"ANTLR\", \"C#\", \"Java\"]\n",
    "mask = ~sums_per_language[\"language\"].isin(keep_languages)\n",
    "sums_per_language.loc[mask, \"language\"] = \"Other\"\n",
    "sums_per_language = sums_per_language.groupby(\"language\", as_index=False).agg({\"files\": \"sum\", \"code\": \"sum\"})\n",
    "sums_per_language = sums_per_language.sort_values(by='code', ascending=False)\n",
    "sums_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Percentage Column\n",
    "total_loc = sums_per_language['code'].sum()\n",
    "sums_per_language['percentage'] = (sums_per_language['code'] / total_loc) * 100\n",
    "\n",
    "\n",
    "# Add the \"Total\" Row\n",
    "total_files = sums_per_language['files'].sum()\n",
    "total_row = pd.DataFrame({\n",
    "    'language': ['Total'],\n",
    "    'files': [total_files],\n",
    "    'code': [total_loc],\n",
    "    'percentage': [100.0]\n",
    "})\n",
    "df_with_total = pd.concat([sums_per_language, total_row], ignore_index=True)\n",
    "\n",
    "# Create a temporary key to define the sort order of row types.\n",
    "# (0 = Language, 1 = Other, 2 = Total)\n",
    "conditions = [\n",
    "    df_with_total['language'] == 'Other',\n",
    "    df_with_total['language'] == 'Total'\n",
    "]\n",
    "choices = [1, 2]\n",
    "df_with_total['sort_key'] = np.select(conditions, choices, default=0)\n",
    "\n",
    "# Sort by the key first (ascending), then by code (descending)\n",
    "df_with_total = df_with_total.sort_values(\n",
    "    by=['sort_key', 'code'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# Remove the temporary key\n",
    "df_with_total = df_with_total.drop(columns='sort_key')\n",
    "\n",
    "# Format for final display\n",
    "display_df = df_with_total.copy()\n",
    "\n",
    "display_df['files'] = display_df['files'].apply(lambda x: f\"{int(x):,}\")\n",
    "display_df['code'] = display_df['code'].apply(lambda x: f\"{int(x):,}\")\n",
    "display_df['percentage'] = display_df['percentage'].apply(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "display_df = display_df.rename(columns={\n",
    "    'language': 'Language',\n",
    "    'files': 'Files',\n",
    "    'code': 'LOC',\n",
    "    'percentage': 'Percentage (%)'\n",
    "})\n",
    "\n",
    "# Display the final result\n",
    "display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LaTeX Table\n",
    "\n",
    "# Make the \"Total\" row bold for emphasis\n",
    "# Find the index of the 'Total' row\n",
    "total_row_index = display_df[display_df['Language'] == 'Total'].index\n",
    "\n",
    "# Wrap each cell in that row with the \\textbf{} command\n",
    "for col in display_df.columns:\n",
    "    display_df.loc[total_row_index, col] = display_df.loc[total_row_index, col].apply(\n",
    "        lambda x: f\"\\\\textbf{{{x}}}\"\n",
    "    )\n",
    "\n",
    "# Generate the LaTeX table string from the DataFrame\n",
    "latex_string = display_df.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Summary of Language Distribution by Lines of Code (LOC).\",\n",
    "    label=\"tab:lang_dist\",\n",
    "    column_format=\"lrrr\", # l=left, r=right alignment for 4 columns\n",
    "    escape=False # Must be False to render the \\textbf command correctly\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(latex_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_final[df_final['language'] == 'SUM']\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the DataFrame to a new CSV file\n",
    "output_file = '../temp_data/revised_sum_per_repo.csv'\n",
    "filtered_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical summary of the 'code' column\n",
    "loc_repos_summary = filtered_df['code'].describe()\n",
    "\n",
    "# maximum\n",
    "max_value = filtered_df['code'].max()\n",
    "\n",
    "# minimum\n",
    "min_value = filtered_df['code'].min()\n",
    "\n",
    "# range (difference between max. and min.)\n",
    "range_value = max_value - min_value\n",
    "\n",
    "# median\n",
    "median_value = filtered_df['code'].median()\n",
    "\n",
    "# variance\n",
    "variance_value = filtered_df['code'].var()\n",
    "\n",
    "# standard deviation\n",
    "std_deviation_value = filtered_df['code'].std()\n",
    "\n",
    "# print results with readable format\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# total number of repositories\n",
    "total_repos = len(filtered_df)\n",
    "\n",
    "# count of repositories with 1000 LOCs or less\n",
    "repos_le_1000 = (filtered_df['code'] <= 1000).sum()\n",
    "\n",
    "# count of repositories with 100 LOCs or less\n",
    "repos_le_100 = (filtered_df['code'] <= 100).sum()\n",
    "\n",
    "# calculate the percentages\n",
    "percentage_le_1000 = (repos_le_1000 / total_repos) * 100\n",
    "percentage_le_100 = (repos_le_100 / total_repos) * 100\n",
    "\n",
    "# display the new results\n",
    "print(f\"\\n% of repos with <= 1000 LOCs: {percentage_le_1000:.2f}%\")\n",
    "print(f\"% of repos with <= 100 LOCs: {percentage_le_100:.2f}%\")\n",
    "\n",
    "# show the results\n",
    "print(\"Statistical summary:\")\n",
    "print(loc_repos_summary)\n",
    "print(\"\\nMax.:\", max_value)\n",
    "print(\"Min.:\", min_value)\n",
    "print(\"Range:\", range_value)\n",
    "print(\"Median:\", median_value)\n",
    "print(\"Variance:\", variance_value)\n",
    "print(\"Standard deviation:\", std_deviation_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the 'code' column\n",
    "loc_repos_data = filtered_df['code']\n",
    "\n",
    "# calculate the IQR\n",
    "Q1 = loc_repos_data.quantile(0.25)\n",
    "Q3 = loc_repos_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the percentage of values within the IQR limits\n",
    "iqr_lower_bound = Q1 - 1.5 * IQR\n",
    "iqr_upper_bound = Q3 + 1.5 * IQR\n",
    "iqr_lower_bound = max(0, iqr_lower_bound)\n",
    "percentage_in_iqr = ((loc_repos_data >= iqr_lower_bound) & (loc_repos_data <= iqr_upper_bound)).mean() * 100\n",
    "\n",
    "# Probability Density Function (PDF)\n",
    "fig_pdf, ax_pdf = plt.subplots(figsize=(10, 6))\n",
    "ax_pdf.hist(loc_repos_data, bins=20, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax_pdf.set_xlabel('LOC', fontsize=12)\n",
    "ax_pdf.set_ylabel('Probability', fontsize=12)\n",
    "ax_pdf.set_title('Probability density function (PDF) for the lines of code (LOC)', fontsize=14)\n",
    "ax_pdf.tick_params(axis='both', labelsize=10)\n",
    "ax_pdf.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax_pdf.text(0.2, 0.7, 'Mean: {:.2f}'.format(loc_repos_data.mean()), transform=ax_pdf.transAxes, fontsize=10, color='purple')\n",
    "ax_pdf.text(0.2, 0.675, 'Standard deviation: {:.2f}'.format(loc_repos_data.std()), transform=ax_pdf.transAxes, fontsize=10, color='purple')\n",
    "ax_pdf.text(0.2, 0.650, f'Data in IQR: {percentage_in_iqr:.2f}%', transform=ax_pdf.transAxes, fontsize=10, color='purple')\n",
    "ax_pdf.axvline(iqr_lower_bound, color='green', linestyle='--', label='IQR lower bound')\n",
    "ax_pdf.axvline(iqr_upper_bound, color='green', linestyle='--', label='IQR upper bound')\n",
    "ax_pdf.legend(loc='upper right', fontsize=10)\n",
    "pdf_file = '../paper/figs/revised_pdf_loc_repos.pdf'\n",
    "fig_pdf.savefig(pdf_file, format='pdf', dpi=300)\n",
    "\n",
    "# Cumulative Density Function (CDF)\n",
    "fig_cdf, ax_cdf = plt.subplots(figsize=(10, 6))\n",
    "sorted_data = np.sort(loc_repos_data)\n",
    "y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "ax_cdf.plot(sorted_data, y, marker='.', linestyle='-', color='red', markersize=4, markeredgecolor='black')\n",
    "ax_cdf.set_xlabel('LOC', fontsize=12)\n",
    "ax_cdf.set_ylabel('Cumulative probability', fontsize=12)\n",
    "ax_cdf.set_title('Cumulative density function (CDF) for the lines of code (LOC)', fontsize=14)\n",
    "ax_cdf.tick_params(axis='both', labelsize=10)\n",
    "ax_cdf.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax_cdf.text(0.2, 0.2, '25th percentile: {:.2f}'.format(np.percentile(sorted_data, 25)), transform=ax_cdf.transAxes, fontsize=10, color='purple')\n",
    "ax_cdf.text(0.2, 0.175, '75th percentile: {:.2f}'.format(np.percentile(sorted_data, 75)), transform=ax_cdf.transAxes, fontsize=10, color='purple')\n",
    "ax_cdf.text(0.2, 0.150, f'Data in IQR: {percentage_in_iqr:.2f}%', transform=ax_cdf.transAxes, fontsize=10, color='purple')\n",
    "ax_cdf.axvline(iqr_lower_bound, color='blue', linestyle='--', label='IQR lower bound')\n",
    "ax_cdf.axvline(iqr_upper_bound, color='blue', linestyle='--', label='IQR upper bound')\n",
    "ax_cdf.legend(loc='lower right', fontsize=10)\n",
    "cdf_file = '../paper/figs/revised_cdf_loc_repos.pdf'\n",
    "fig_cdf.savefig(cdf_file, format='pdf', dpi=300)\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 1 subplot\n",
    "fig, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# calculate and graph the CDF\n",
    "sorted_data = np.sort(loc_repos_data)\n",
    "y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "ax2.plot(sorted_data, y, color='blue', linewidth=2)\n",
    "ax2.set_xlabel('LOC', fontsize=22)  # Increased font size\n",
    "ax2.set_ylabel('Cumulative probability', fontsize=22)  # Increased font size\n",
    "ax2.set_title('LOC cumulative distribution function (CDF)', fontsize=22)  # Increased font size\n",
    "\n",
    "# customize graphs\n",
    "ax2.tick_params(axis='both', labelsize=20)  # Increased tick label font size\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# adjust scale to log\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "# adjust spacing between subplots and labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plots as image files\n",
    "df_file = '../paper/figs/revised_cdf_loc_repos_log.pdf'\n",
    "plt.savefig(df_file, dpi=300)  # adjust DPI for high res. outputs\n",
    "\n",
    "# shows the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate DFs from input CSVs\n",
    "input_file_1 = '../temp_data/revised_loc_analysis.csv'\n",
    "input_file_2 = '../temp_data/event_counts_per_repo.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(input_file_1)\n",
    "columns_to_remove = ['language', 'files', 'blank', 'comment']\n",
    "df1 = df1.drop(columns=columns_to_remove)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers from df1\n",
    "loc_data = df1['code']\n",
    "\n",
    "# calculate the interquartile range (IQR)\n",
    "Q1 = loc_data.quantile(0.25)\n",
    "Q3 = loc_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the percentage of values within the IQR limits\n",
    "iqr_lower_bound = Q1 - 1.5 * IQR\n",
    "iqr_upper_bound = Q3 + 1.5 * IQR\n",
    "iqr_lower_bound = max(0, iqr_lower_bound)\n",
    "print(f\"Lower bound: {iqr_lower_bound}, Upper bound: {iqr_upper_bound}\")\n",
    "\n",
    "df1 = df1[(df1['code'] >= iqr_lower_bound) & (df1['code'] <= iqr_upper_bound)]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(input_file_2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove outliers from df2\n",
    "event_count_data = df2['count']\n",
    "\n",
    "# calculate the interquartile range (IQR)\n",
    "Q1 = event_count_data.quantile(0.25)\n",
    "Q3 = event_count_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# calculate the percentage of values within the IQR limits, ensuring the lower limit is not less than 0\n",
    "iqr_lower_bound = Q1 - 1.5 * IQR\n",
    "iqr_upper_bound = Q3 + 1.5 * IQR\n",
    "iqr_lower_bound = max(0, iqr_lower_bound)\n",
    "print(f\"Lower bound: {iqr_lower_bound}, Upper bound: {iqr_upper_bound}\")\n",
    "\n",
    "df2 = df2[(df2['count'] >= iqr_lower_bound) & (df2['count'] <= iqr_upper_bound)]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df1, df2, left_on='project_id', right_on='project_id', how='inner')\n",
    "merged_df = merged_df.drop(columns=['project_id'])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation between 'code' and 'count'\n",
    "correlation = merged_df['code'].corr(merged_df['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_context(\"notebook\", font_scale=1.5)  # Adjust font_scale to increase font size\n",
    "sns.scatterplot(data=merged_df, x='code', y='count')\n",
    "plt.title(f'Number of event triggers vs. LOC (correlation: {correlation:.2f})', fontsize=18)\n",
    "plt.xlabel('LOC' , fontsize=16)\n",
    "plt.ylabel('No. of event triggers', fontsize=16)\n",
    "\n",
    "# save the plot as a PDF\n",
    "scatter_plot_pdf_file = '../paper/figs/revised_scatter_plot_event_triggers_vs_loc.pdf'\n",
    "plt.savefig(scatter_plot_pdf_file, format='pdf', dpi=300)\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openlambdaverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
