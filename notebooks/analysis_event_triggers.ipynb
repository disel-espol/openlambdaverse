{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook collects the prevalence of event trigger types across all repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent code search directory in data/processed, and obtain the final JSONL file path\n",
    "PROCESSED_DATA_DIR = os.path.join(os.getcwd(), \"..\", \"data\", \"processed\")\n",
    "latest_dir_pattern = os.path.join(PROCESSED_DATA_DIR, \"code_search_*\")\n",
    "latest_dir = max(glob.glob(latest_dir_pattern), key=os.path.getmtime, default=None)\n",
    "\n",
    "if latest_dir:\n",
    "    RESULTS_DIR = os.path.join(latest_dir, \"results\")\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No matching code_search_YYYYMMDD_hhmmss directory found.\")\n",
    "\n",
    "INPUT_FILENAME = os.path.join(RESULTS_DIR, \"aws_provider_repos.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load that JSONL file\n",
    "rows = []\n",
    "with open(INPUT_FILENAME, \"r\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "# And we extract the events from each function in each repo\n",
    "events = []\n",
    "for repo in rows:\n",
    "    project_id = repo.get(\"repository\")\n",
    "    serverless_config = repo.get(\"serverless_config\", [])\n",
    "    # serverless_config is a list of dicts, each with a 'config' key\n",
    "    for config_obj in serverless_config:\n",
    "        config = config_obj.get(\"config\", {})\n",
    "        functions = config.get(\"events\") or config.get(\"functions\", {})\n",
    "        if isinstance(functions, dict):\n",
    "            for function_name, function_data in functions.items():\n",
    "                if function_data is None or isinstance(function_data, str):\n",
    "                    events.append([project_id, function_name, \"N/A\"])\n",
    "                    continue\n",
    "                event_list = function_data.get(\"events\", [])\n",
    "                if isinstance(event_list, list):\n",
    "                    for event_dict in event_list:\n",
    "                        if isinstance(event_dict, dict):\n",
    "                            for event_type in event_dict.keys():\n",
    "                                events.append([project_id, function_name, event_type])\n",
    "                        else:\n",
    "                            events.append([project_id, function_name, str(event_dict)])\n",
    "                elif isinstance(event_list, dict):\n",
    "                    for event_type in event_list.keys():\n",
    "                        events.append([project_id, function_name, event_type])\n",
    "                else:\n",
    "                    events.append([project_id, function_name, \"N/A\"])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(events, columns=[\"project_id\", \"function_name\", \"event\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.dropna(subset=['event'])\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events supported by the Serverless Framework\n",
    "supported_events = [\n",
    "'httpApi',\n",
    "'http',\n",
    "'activemq',\n",
    "'alb',\n",
    "'alexaSkill',\n",
    "'alexaSmartHome',\n",
    "'cloudwatchEvent',\n",
    "'cloudwatchLog',\n",
    "'cloudFront',\n",
    "'cognitoUserPool',\n",
    "'eventBridge',\n",
    "'iot',\n",
    "'iotFleetProvisioning',\n",
    "'kafka',\n",
    "'stream',\n",
    "'msk',\n",
    "'rabbitmq',\n",
    "'s3',\n",
    "'schedule',\n",
    "'sns',\n",
    "'sqs',\n",
    "'websocket'\n",
    "]\n",
    "supported_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[filtered_df['event'].isin(supported_events)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the filtered DataFrame by the 'event' column\n",
    "grouped_df = filtered_df.groupby('event').agg({\n",
    "    'project_id': 'count'  # Count of occurrences\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "grouped_df = grouped_df.rename(columns={'project_id': 'count'})\n",
    "\n",
    "# Calculate the total count of runtimes\n",
    "total_count = grouped_df['count'].sum()\n",
    "\n",
    "# Add a new column \"occurrence\" with the percentage values\n",
    "grouped_df['occurrence'] = (grouped_df['count'] / total_count) * 100\n",
    "\n",
    "# Sort df by the \"occurrence\" column in descending order\n",
    "grouped_df = grouped_df.sort_values(by='occurrence', ascending=False)\n",
    "\n",
    "# Reset the index\n",
    "grouped_df = grouped_df.reset_index(drop=True)\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the filtered DataFrame by the 'event' column\n",
    "grouped_df = filtered_df.groupby('event').agg({\n",
    "    'project_id': 'count'  # Count of occurrences (event triggers)\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "grouped_df = grouped_df.rename(columns={'project_id': 'count'})\n",
    "\n",
    "# Calculate the total count of event triggers\n",
    "total_count = grouped_df['count'].sum()\n",
    "\n",
    "# Add a new column \"occurrence\" with the percentage values (event triggers)\n",
    "grouped_df['occurrence'] = (grouped_df['count'] / total_count) * 100\n",
    "\n",
    "# Calculate the percentage of repositories in which each event appears\n",
    "repo_counts = filtered_df.groupby('event')['project_id'].nunique()\n",
    "total_repos = filtered_df['project_id'].nunique()\n",
    "grouped_df['repo_percentage'] = grouped_df['event'].map(lambda e: (repo_counts[e] / total_repos) * 100)\n",
    "\n",
    "# Sort df by the \"occurrence\" column in descending order\n",
    "grouped_df = grouped_df.sort_values(by='occurrence', ascending=False)\n",
    "\n",
    "# Reset the index\n",
    "grouped_df = grouped_df.reset_index(drop=True)\n",
    "grouped_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openlambdaverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
