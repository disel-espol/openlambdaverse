{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook collects the prevalence of event trigger types across all repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent code search directory in data/processed, and obtain the final JSONL file path\n",
    "PROCESSED_DATA_DIR = os.path.join(os.getcwd(), \"..\", \"data\", \"processed\")\n",
    "latest_dir_pattern = os.path.join(PROCESSED_DATA_DIR, \"code_search_*\")\n",
    "latest_dir = max(glob.glob(latest_dir_pattern), key=os.path.getmtime, default=None)\n",
    "\n",
    "if latest_dir:\n",
    "    RESULTS_DIR = os.path.join(latest_dir, \"results\")\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No matching code_search_YYYYMMDD_hhmmss directory found.\")\n",
    "\n",
    "INPUT_FILENAME = os.path.join(RESULTS_DIR, \"aws_provider_repos.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load that JSONL file\n",
    "rows = []\n",
    "with open(INPUT_FILENAME, \"r\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "# And we extract the events from each function in each repo\n",
    "events = []\n",
    "for repo in rows:\n",
    "    project_id = repo.get(\"repository\")\n",
    "    serverless_config = repo.get(\"serverless_config\", [])\n",
    "    # serverless_config is a list of dicts, each with a 'config' key\n",
    "    for config_obj in serverless_config:\n",
    "        config = config_obj.get(\"config\", {})\n",
    "        functions = config.get(\"events\") or config.get(\"functions\", {})\n",
    "        if isinstance(functions, dict):\n",
    "            for function_name, function_data in functions.items():\n",
    "                if function_data is None or isinstance(function_data, str):\n",
    "                    events.append([project_id, function_name, \"N/A\"])\n",
    "                    continue\n",
    "                event_list = function_data.get(\"events\", [])\n",
    "                if isinstance(event_list, list):\n",
    "                    for event_dict in event_list:\n",
    "                        if isinstance(event_dict, dict):\n",
    "                            for event_type in event_dict.keys():\n",
    "                                events.append([project_id, function_name, event_type])\n",
    "                        else:\n",
    "                            events.append([project_id, function_name, str(event_dict)])\n",
    "                elif isinstance(event_list, dict):\n",
    "                    for event_type in event_list.keys():\n",
    "                        events.append([project_id, function_name, event_type])\n",
    "                else:\n",
    "                    events.append([project_id, function_name, \"N/A\"])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(events, columns=[\"project_id\", \"function_name\", \"event\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.dropna(subset=['event'])\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events supported by the Serverless Framework\n",
    "supported_events = [\n",
    "'httpApi',\n",
    "'http',\n",
    "'activemq',\n",
    "'alb',\n",
    "'alexaSkill',\n",
    "'alexaSmartHome',\n",
    "'cloudwatchEvent',\n",
    "'cloudwatchLog',\n",
    "'cloudFront',\n",
    "'cognitoUserPool',\n",
    "'eventBridge',\n",
    "'iot',\n",
    "'iotFleetProvisioning',\n",
    "'kafka',\n",
    "'stream',\n",
    "'msk',\n",
    "'rabbitmq',\n",
    "'s3',\n",
    "'schedule',\n",
    "'sns',\n",
    "'sqs',\n",
    "'websocket'\n",
    "]\n",
    "supported_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[filtered_df['event'].isin(supported_events)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the filtered DataFrame by the 'event' column\n",
    "grouped_df = filtered_df.groupby('event').agg({\n",
    "    'project_id': 'count'  # Count of occurrences\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "grouped_df = grouped_df.rename(columns={'project_id': 'count'})\n",
    "\n",
    "# Calculate the total count of runtimes\n",
    "total_count = grouped_df['count'].sum()\n",
    "\n",
    "# Add a new column \"occurrence\" with the percentage values\n",
    "grouped_df['occurrence'] = (grouped_df['count'] / total_count) * 100\n",
    "\n",
    "# Sort df by the \"occurrence\" column in descending order\n",
    "grouped_df = grouped_df.sort_values(by='occurrence', ascending=False)\n",
    "\n",
    "# Reset the index\n",
    "grouped_df = grouped_df.reset_index(drop=True)\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the filtered DataFrame by the 'event' column\n",
    "grouped_df = filtered_df.groupby('event').agg({\n",
    "    'project_id': 'count'  # Count of occurrences (event triggers)\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "grouped_df = grouped_df.rename(columns={'project_id': 'count'})\n",
    "\n",
    "# Calculate the total count of event triggers\n",
    "total_count = grouped_df['count'].sum()\n",
    "\n",
    "# Add a new column \"occurrence\" with the percentage values (event triggers)\n",
    "grouped_df['occurrence'] = (grouped_df['count'] / total_count) * 100\n",
    "\n",
    "# Calculate the percentage of repositories in which each event appears\n",
    "repo_counts = filtered_df.groupby('event')['project_id'].nunique()\n",
    "total_repos = filtered_df['project_id'].nunique()\n",
    "grouped_df['repo_percentage'] = grouped_df['event'].map(lambda e: (repo_counts[e] / total_repos) * 100)\n",
    "\n",
    "# Sort df by the \"occurrence\" column in descending order\n",
    "grouped_df = grouped_df.sort_values(by='occurrence', ascending=False)\n",
    "\n",
    "# Reset the index\n",
    "grouped_df = grouped_df.reset_index(drop=True)\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_counts_df = df.drop(columns=['function_name'])\n",
    "event_counts_df['common_id'] = event_counts_df['project_id'].str.replace(r'_\\d+$', '', regex=True)\n",
    "event_counts_df = event_counts_df.groupby(['common_id', 'event']).size().reset_index(name='count')\n",
    "event_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = event_counts_df.groupby('common_id')['count'].sum().reset_index()\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of the 'count' column\n",
    "events_count_summary = grouped_df['count'].describe()\n",
    "\n",
    "# Maximum value\n",
    "max_value = grouped_df['count'].max()\n",
    "\n",
    "# Minimum value\n",
    "min_value = grouped_df['count'].min()\n",
    "\n",
    "# Range (difference between maximum and minimum)\n",
    "range_value = max_value - min_value\n",
    "\n",
    "# Median\n",
    "median_value = grouped_df['count'].median()\n",
    "\n",
    "# Variance\n",
    "variance_value = grouped_df['count'].var()\n",
    "\n",
    "# Standard deviation\n",
    "std_deviation_value = grouped_df['count'].std()\n",
    "\n",
    "# Cleaner display of floating point numbers\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Show the results\n",
    "print(\"Statistical analysis:\")\n",
    "print(events_count_summary)\n",
    "print(\"\\nMax.:\", max_value)\n",
    "print(\"Min.:\", min_value)\n",
    "print(\"Range:\", range_value)\n",
    "print(\"Median:\", median_value)\n",
    "print(\"Variance:\", variance_value)\n",
    "print(\"Standard deviation:\", std_deviation_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'count' data for plotting\n",
    "events_count_data = grouped_df['count']\n",
    "\n",
    "# Calculate the interquartile range (IQR)\n",
    "Q1 = events_count_data.quantile(0.25)\n",
    "Q3 = events_count_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculate the percentage of values within the IQR bounds\n",
    "iqr_lower_bound = Q1 - 1.5 * IQR\n",
    "iqr_upper_bound = Q3 + 1.5 * IQR\n",
    "iqr_lower_bound = max(0, iqr_lower_bound) # IQR lower bound cannot be negative\n",
    "percentage_in_iqr = ((events_count_data >= iqr_lower_bound) & (events_count_data <= iqr_upper_bound)).mean() * 100\n",
    "\n",
    "# PDF plot\n",
    "fig_pdf, ax_pdf = plt.subplots(figsize=(10, 6))\n",
    "ax_pdf.hist(events_count_data, bins=20, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax_pdf.set_xlabel('Number of event triggers', fontsize=12)\n",
    "ax_pdf.set_ylabel('Probability', fontsize=12)\n",
    "ax_pdf.set_title('Probability density function (PDF) for the number of event triggers', fontsize=14)\n",
    "ax_pdf.axvline(iqr_lower_bound, color='green', linestyle='--', label='IQR lower bound')\n",
    "ax_pdf.axvline(iqr_upper_bound, color='green', linestyle='--', label='IQR upper bound')\n",
    "ax_pdf.legend(loc='upper right', fontsize=10)\n",
    "ax_pdf.tick_params(axis='both', labelsize=10)\n",
    "ax_pdf.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax_pdf.text(0.2, 0.7, 'Mean: {:.2f}'.format(events_count_data.mean()), transform=ax_pdf.transAxes, fontsize=10, color='purple')\n",
    "ax_pdf.text(0.2, 0.675, 'Standard deviation: {:.2f}'.format(events_count_data.std()), transform=ax_pdf.transAxes, fontsize=10, color='purple')\n",
    "ax_pdf.text(0.2, 0.650, f'Data in IQR: {percentage_in_iqr:.2f}%', transform=ax_pdf.transAxes, fontsize=10, color='purple')\n",
    "pdf_file = '../paper/figs/pdf_events_count_data.pdf'\n",
    "fig_pdf.savefig(pdf_file, format='pdf', dpi=300)\n",
    "\n",
    "# CDF plot with increased font sizes\n",
    "fig_cdf, ax_cdf = plt.subplots(figsize=(10, 6))\n",
    "sorted_data = np.sort(events_count_data)\n",
    "y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "ax_cdf.plot(sorted_data, y, marker='.', linestyle='-', color='red', markersize=4, markeredgecolor='black')\n",
    "ax_cdf.set_xlabel('Number of event triggers', fontsize=22)  # Increased font size\n",
    "ax_cdf.set_ylabel('Cumulative probability', fontsize=22)  # Increased font size\n",
    "ax_cdf.set_title('CDF of the number of event triggers per repo.', fontsize=22)  # Increased font size\n",
    "ax_cdf.axvline(iqr_lower_bound, color='blue', linestyle='--', label='IQR lower bound')\n",
    "ax_cdf.axvline(iqr_upper_bound, color='blue', linestyle='--', label='IQR upper bound')\n",
    "ax_cdf.legend(loc='lower right', fontsize=16)  # Increased font size\n",
    "ax_cdf.tick_params(axis='both', labelsize=16)  # Increased tick label font size\n",
    "ax_cdf.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax_cdf.text(0.4, 0.6, '25th percentile: {:.2f}'.format(np.percentile(sorted_data, 25)), transform=ax_cdf.transAxes, fontsize=16, color='purple')  # Increased font size\n",
    "ax_cdf.text(0.4, 0.55, '75th percentile: {:.2f}'.format(np.percentile(sorted_data, 75)), transform=ax_cdf.transAxes, fontsize=16, color='purple')  # Increased font size\n",
    "ax_cdf.text(0.4, 0.5, f'Data in IQR: {percentage_in_iqr:.2f}%', transform=ax_cdf.transAxes, fontsize=16, color='purple')  # Increased font size\n",
    "cdf_file = '../paper/figs/cdf_events_count_data.pdf'\n",
    "fig_cdf.savefig(cdf_file, format='pdf', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openlambdaverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
